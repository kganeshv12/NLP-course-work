{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z1NM-FRQcWnf"
   },
   "outputs": [],
   "source": [
    "#NLP UNIT 1 HANDS ON SESSION(COURSE CODE:UE20CS334)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Xm0CBlQcjIn",
    "outputId": "e9199ab0-b2c6-45c9-be16-ff139a5e66da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install nltk\n",
    "#!pip install spacy\n",
    "#!pip install pattern\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kgpnr4hlcjCm",
    "outputId": "3fe943a9-813e-4eb6-dfb4-63d859fe5fe2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IMPORT NECESSARY LIBRARIES\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYwACyWmeE6Y"
   },
   "source": [
    "**Sentence Tokenisation**\n",
    "\n",
    "*Sentence boundary detection*, as its name suggests, addresses the problem of finding sentence boundaries. The concept of sentence is central in several natural language processing tasks, since sentences are standard textual units which confine a variety of linguistic phenomena such as collocations and variable binding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MnEkSvHsdV-A",
    "outputId": "9d70ee5c-3276-41f9-e3b3-53c5cefdce51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awesome!\n",
      "I am learning NLP.\n"
     ]
    }
   ],
   "source": [
    "#Sentence Tokenisation using NLTK to print individual sentences.\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"Awesome! I am learning NLP.\"\n",
    "for sent in sent_tokenize(text):\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qMncA8LJciL7",
    "outputId": "dc5176ce-850c-4f0b-dbdd-d49b81dce65d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world.\n",
      "Here are two sentences.\n"
     ]
    }
   ],
   "source": [
    "#Sentence Tokenisation using Spacy to print individual sentences.\n",
    "\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "nlp.add_pipe('sentencizer')\n",
    "doc = nlp('Hello, world. Here are two sentences.')\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWWQ13cuxacj"
   },
   "source": [
    "**Chunking**\n",
    "\n",
    "This method is generally an optional method in the preprocessing of text. Chunking tries to split the sentences further into more meaningful pieces. For example the statement My name is Michael, I live in Bangalore. We can split this sentence into 2 parts one that provides information on the identity and the other on habitat. This is generally achieved through Semantic Segmentation using POS tagging and building syntactic trees that can then be split on the bases of need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "gt5iwfJ8dt4U",
    "outputId": "4f5de311-9258-49e2-8754-1d6f4d8be93b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"09d0c7898a224bcf88f4dbd9d8f4dce1-0\" class=\"displacy\" width=\"1170\" height=\"277.0\" direction=\"ltr\" style=\"max-width: none; height: 277.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"187.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">My</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"187.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"190\">name</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"190\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"187.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"330\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"330\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"187.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"470\">Michael,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"470\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"187.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"610\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"610\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"187.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">live</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"187.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"890\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"890\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"187.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1030\">Bangalore.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1030\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-09d0c7898a224bcf88f4dbd9d8f4dce1-0-0\" stroke-width=\"2px\" d=\"M70,142.0 C70,72.0 185.0,72.0 185.0,142.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-09d0c7898a224bcf88f4dbd9d8f4dce1-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,144.0 L62,132.0 78,132.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-09d0c7898a224bcf88f4dbd9d8f4dce1-0-1\" stroke-width=\"2px\" d=\"M210,142.0 C210,72.0 325.0,72.0 325.0,142.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-09d0c7898a224bcf88f4dbd9d8f4dce1-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M210,144.0 L202,132.0 218,132.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-09d0c7898a224bcf88f4dbd9d8f4dce1-0-2\" stroke-width=\"2px\" d=\"M350,142.0 C350,2.0 750.0,2.0 750.0,142.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-09d0c7898a224bcf88f4dbd9d8f4dce1-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M350,144.0 L342,132.0 358,132.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-09d0c7898a224bcf88f4dbd9d8f4dce1-0-3\" stroke-width=\"2px\" d=\"M350,142.0 C350,72.0 465.0,72.0 465.0,142.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-09d0c7898a224bcf88f4dbd9d8f4dce1-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M465.0,144.0 L473.0,132.0 457.0,132.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-09d0c7898a224bcf88f4dbd9d8f4dce1-0-4\" stroke-width=\"2px\" d=\"M630,142.0 C630,72.0 745.0,72.0 745.0,142.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-09d0c7898a224bcf88f4dbd9d8f4dce1-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M630,144.0 L622,132.0 638,132.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-09d0c7898a224bcf88f4dbd9d8f4dce1-0-5\" stroke-width=\"2px\" d=\"M770,142.0 C770,72.0 885.0,72.0 885.0,142.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-09d0c7898a224bcf88f4dbd9d8f4dce1-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M885.0,144.0 L893.0,132.0 877.0,132.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-09d0c7898a224bcf88f4dbd9d8f4dce1-0-6\" stroke-width=\"2px\" d=\"M910,142.0 C910,72.0 1025.0,72.0 1025.0,142.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-09d0c7898a224bcf88f4dbd9d8f4dce1-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1025.0,144.0 L1033.0,132.0 1017.0,132.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Chunking using Spacy\n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp('My name is Michael, I live in Bangalore.')\n",
    "displacy.render(doc,style='dep',jupyter=True,options={'distance':140})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4EfKTufxmza"
   },
   "source": [
    "**Word Tokenisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c6I1s-LNxgAx",
    "outputId": "364567c4-b1c0-46eb-d531-91a1f0a088ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "God\n",
      "is\n",
      "Great\n",
      "!\n",
      "I\n",
      "won\n",
      "a\n",
      "lottery\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "#Word Tokenisation using spacy to print individual words\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"God is Great! I won a lottery.\"\n",
    "for word in word_tokenize(text):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iZkPCEwVxgEI",
    "outputId": "168cf02d-db9b-4760-83e8-8470ea3a081b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "God\n",
      "is\n",
      "Great\n",
      "!\n",
      "I\n",
      "won\n",
      "a\n",
      "lottery\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "#Word Tokenisation using spacy to print individual words\n",
    "\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "# Created by processing a string of text with the nlp object\n",
    "doc = nlp(\"God is Great! I won a lottery.\")\n",
    "\n",
    "# Iterate over tokens in a Doc\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "za3ar7xdySTA"
   },
   "source": [
    "**Morphological Analysis and Part-of-Speech Tagging**\n",
    "\n",
    "Having texts separated in tokens, the next step is usually morphosyntactic analysis, in order to identify characteristics as word lemma and parts of speech.\n",
    "\n",
    "It is important to distinguish two concepts: *lexeme and word form.*\n",
    "\n",
    "The process of determining the word lemma is called *lemmatization.*\n",
    "\n",
    "Another method called **word stemming** is common due to its simplicity. Word stemming reduces words to their base form by removing suffixes. The remaining form is not necessarily a valid root but it is usually suffi cient that related words map to the same stem or to a reduced set of stems if words are irregular. For example the words “mice” and “mouse” have the lemma “mouse” but some stemmers produce “mic” and “mous”, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQjVE95zqxL3"
   },
   "source": [
    "**NLTK LEMMATIZER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vBHKOvSgx7cU",
    "outputId": "45a88db3-89b5-49cf-9139-a6ca538cc9f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bat\n",
      "are\n",
      "foot\n",
      "['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best', 'support']\n",
      "The striped bat are hanging on their foot for best support\n"
     ]
    }
   ],
   "source": [
    "#NLTK Lemmatization\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize Single Word\n",
    "print(lemmatizer.lemmatize(\"bats\"))\n",
    "print(lemmatizer.lemmatize(\"are\"))\n",
    "print(lemmatizer.lemmatize(\"feet\"))\n",
    "\n",
    "# Define the sentence to be lemmatized\n",
    "sentence = \"The striped bats are hanging on their feet for best support\"\n",
    "\n",
    "# Tokenize: Split the sentence into words\n",
    "word_list = nltk.word_tokenize(sentence)\n",
    "print(word_list)\n",
    "\n",
    "# Lemmatize list of words and join\n",
    "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "print(lemmatized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zGgjlkdKyrhf",
    "outputId": "237dba58-d651-41f7-de80-c0b3a17bf271"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strip\n",
      "stripe\n"
     ]
    }
   ],
   "source": [
    "#LEMMAS BASED ON CONTEXT(VERB/NOUN)\n",
    "\n",
    "print(lemmatizer.lemmatize(\"stripes\", 'v'))\n",
    "print(lemmatizer.lemmatize(\"stripes\", 'n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5x6I2KwqtjH"
   },
   "source": [
    "**SPACY LEMMATIZER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Pkomt7k5yzd6",
    "outputId": "7ac0ec72-cc79-4fb2-b045-e4a77636601b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'the stripe bat be hang on their foot for good support'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SPACY Lemmatization\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "sentence = \"The striped bats are hanging on their feet for best support\"\n",
    "\n",
    "# Parse the sentence using the loaded 'en' model object `nlp`\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Extract the lemma for each token and join\n",
    "\" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LR9bmrCCqq1Y"
   },
   "source": [
    "**TEXTBLOB LEMMATIZER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "UPPE_opPzLEr",
    "outputId": "4a58254e-0f9d-4f51-9fc6-2f571cc0ef09"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'stripe'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TEXTBLOB Lemmatization\n",
    "from textblob import TextBlob, Word\n",
    "\n",
    "# Lemmatize a word\n",
    "word = 'stripes'\n",
    "w = Word(word)\n",
    "w.lemmatize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCrW2b_Vqk-H"
   },
   "source": [
    "**PATTERN LEMMATIZER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6AzNKp9qhul"
   },
   "outputs": [],
   "source": [
    "import pattern\n",
    "from pattern.en import lemma, lexeme\n",
    "\n",
    "sentence = \"The striped bats were hanging on their feet and ate best fishes\"\" \".join([lemma(wd) for wd in sentence.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SZ7DPL2hq2kb",
    "outputId": "d334e312-f3c3-4d32-81d4-8e952f8cc35d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['thethe', 'thethes', 'thething', 'thethed'],\n",
       " ['stripe', 'stripes', 'striping', 'striped'],\n",
       " ['bat', 'bats', 'batting', 'batted'],\n",
       " ['be',\n",
       "  'am',\n",
       "  'are',\n",
       "  'is',\n",
       "  'being',\n",
       "  'was',\n",
       "  'were',\n",
       "  'been',\n",
       "  'am not',\n",
       "  \"aren't\",\n",
       "  \"isn't\",\n",
       "  \"wasn't\",\n",
       "  \"weren't\"],\n",
       " ['hang', 'hangs', 'hanging', 'hung'],\n",
       " ['on', 'ons', 'oning', 'oned'],\n",
       " ['their', 'theirs', 'theiring', 'theired'],\n",
       " ['feet', 'feets', 'feeting', 'feeted'],\n",
       " ['and', 'ands', 'anding', 'anded'],\n",
       " ['eat', 'eats', 'eating', 'ate', 'eaten'],\n",
       " ['best', 'bests', 'besting', 'bested'],\n",
       " ['fishes', 'fishing', 'fishesed'],\n",
       " ['stripethe', 'stripethes', 'stripething', 'stripethed'],\n",
       " ['stripe', 'stripes', 'striping', 'striped'],\n",
       " ['bat', 'bats', 'batting', 'batted'],\n",
       " ['be',\n",
       "  'am',\n",
       "  'are',\n",
       "  'is',\n",
       "  'being',\n",
       "  'was',\n",
       "  'were',\n",
       "  'been',\n",
       "  'am not',\n",
       "  \"aren't\",\n",
       "  \"isn't\",\n",
       "  \"wasn't\",\n",
       "  \"weren't\"],\n",
       " ['hang', 'hangs', 'hanging', 'hung'],\n",
       " ['on', 'ons', 'oning', 'oned'],\n",
       " ['their', 'theirs', 'theiring', 'theired'],\n",
       " ['feet', 'feets', 'feeting', 'feeted'],\n",
       " ['and', 'ands', 'anding', 'anded'],\n",
       " ['eat', 'eats', 'eating', 'ate', 'eaten'],\n",
       " ['best', 'bests', 'besting', 'bested'],\n",
       " ['fishes', 'fishing', 'fishesed'],\n",
       " ['batthe', 'batthes', 'batthing', 'batthed'],\n",
       " ['stripe', 'stripes', 'striping', 'striped'],\n",
       " ['bat', 'bats', 'batting', 'batted'],\n",
       " ['be',\n",
       "  'am',\n",
       "  'are',\n",
       "  'is',\n",
       "  'being',\n",
       "  'was',\n",
       "  'were',\n",
       "  'been',\n",
       "  'am not',\n",
       "  \"aren't\",\n",
       "  \"isn't\",\n",
       "  \"wasn't\",\n",
       "  \"weren't\"],\n",
       " ['hang', 'hangs', 'hanging', 'hung'],\n",
       " ['on', 'ons', 'oning', 'oned'],\n",
       " ['their', 'theirs', 'theiring', 'theired'],\n",
       " ['feet', 'feets', 'feeting', 'feeted'],\n",
       " ['and', 'ands', 'anding', 'anded'],\n",
       " ['eat', 'eats', 'eating', 'ate', 'eaten'],\n",
       " ['best', 'bests', 'besting', 'bested'],\n",
       " ['fishes', 'fishing', 'fishesed'],\n",
       " ['bethe', 'bethes', 'bething', 'bethed'],\n",
       " ['stripe', 'stripes', 'striping', 'striped'],\n",
       " ['bat', 'bats', 'batting', 'batted'],\n",
       " ['be',\n",
       "  'am',\n",
       "  'are',\n",
       "  'is',\n",
       "  'being',\n",
       "  'was',\n",
       "  'were',\n",
       "  'been',\n",
       "  'am not',\n",
       "  \"aren't\",\n",
       "  \"isn't\",\n",
       "  \"wasn't\",\n",
       "  \"weren't\"],\n",
       " ['hang', 'hangs', 'hanging', 'hung'],\n",
       " ['on', 'ons', 'oning', 'oned'],\n",
       " ['their', 'theirs', 'theiring', 'theired'],\n",
       " ['feet', 'feets', 'feeting', 'feeted'],\n",
       " ['and', 'ands', 'anding', 'anded'],\n",
       " ['eat', 'eats', 'eating', 'ate', 'eaten'],\n",
       " ['best', 'bests', 'besting', 'bested'],\n",
       " ['fishes', 'fishing', 'fishesed'],\n",
       " ['hangthe', 'hangthes', 'hangthing', 'hangthed'],\n",
       " ['stripe', 'stripes', 'striping', 'striped'],\n",
       " ['bat', 'bats', 'batting', 'batted'],\n",
       " ['be',\n",
       "  'am',\n",
       "  'are',\n",
       "  'is',\n",
       "  'being',\n",
       "  'was',\n",
       "  'were',\n",
       "  'been',\n",
       "  'am not',\n",
       "  \"aren't\",\n",
       "  \"isn't\",\n",
       "  \"wasn't\",\n",
       "  \"weren't\"],\n",
       " ['hang', 'hangs', 'hanging', 'hung'],\n",
       " ['on', 'ons', 'oning', 'oned'],\n",
       " ['their', 'theirs', 'theiring', 'theired'],\n",
       " ['feet', 'feets', 'feeting', 'feeted'],\n",
       " ['and', 'ands', 'anding', 'anded'],\n",
       " ['eat', 'eats', 'eating', 'ate', 'eaten'],\n",
       " ['best', 'bests', 'besting', 'bested'],\n",
       " ['fishes', 'fishing', 'fishesed'],\n",
       " ['onthe', 'onthes', 'onthing', 'onthed'],\n",
       " ['stripe', 'stripes', 'striping', 'striped'],\n",
       " ['bat', 'bats', 'batting', 'batted'],\n",
       " ['be',\n",
       "  'am',\n",
       "  'are',\n",
       "  'is',\n",
       "  'being',\n",
       "  'was',\n",
       "  'were',\n",
       "  'been',\n",
       "  'am not',\n",
       "  \"aren't\",\n",
       "  \"isn't\",\n",
       "  \"wasn't\",\n",
       "  \"weren't\"],\n",
       " ['hang', 'hangs', 'hanging', 'hung'],\n",
       " ['on', 'ons', 'oning', 'oned'],\n",
       " ['their', 'theirs', 'theiring', 'theired'],\n",
       " ['feet', 'feets', 'feeting', 'feeted'],\n",
       " ['and', 'ands', 'anding', 'anded'],\n",
       " ['eat', 'eats', 'eating', 'ate', 'eaten'],\n",
       " ['best', 'bests', 'besting', 'bested'],\n",
       " ['fishes', 'fishing', 'fishesed'],\n",
       " ['theirthe', 'theirthes', 'theirthing', 'theirthed'],\n",
       " ['stripe', 'stripes', 'striping', 'striped'],\n",
       " ['bat', 'bats', 'batting', 'batted'],\n",
       " ['be',\n",
       "  'am',\n",
       "  'are',\n",
       "  'is',\n",
       "  'being',\n",
       "  'was',\n",
       "  'were',\n",
       "  'been',\n",
       "  'am not',\n",
       "  \"aren't\",\n",
       "  \"isn't\",\n",
       "  \"wasn't\",\n",
       "  \"weren't\"],\n",
       " ['hang', 'hangs', 'hanging', 'hung'],\n",
       " ['on', 'ons', 'oning', 'oned'],\n",
       " ['their', 'theirs', 'theiring', 'theired'],\n",
       " ['feet', 'feets', 'feeting', 'feeted'],\n",
       " ['and', 'ands', 'anding', 'anded'],\n",
       " ['eat', 'eats', 'eating', 'ate', 'eaten'],\n",
       " ['best', 'bests', 'besting', 'bested'],\n",
       " ['fishes', 'fishing', 'fishesed'],\n",
       " ['feetthe', 'feetthes', 'feetthing', 'feetthed'],\n",
       " ['stripe', 'stripes', 'striping', 'striped'],\n",
       " ['bat', 'bats', 'batting', 'batted'],\n",
       " ['be',\n",
       "  'am',\n",
       "  'are',\n",
       "  'is',\n",
       "  'being',\n",
       "  'was',\n",
       "  'were',\n",
       "  'been',\n",
       "  'am not',\n",
       "  \"aren't\",\n",
       "  \"isn't\",\n",
       "  \"wasn't\",\n",
       "  \"weren't\"],\n",
       " ['hang', 'hangs', 'hanging', 'hung'],\n",
       " ['on', 'ons', 'oning', 'oned'],\n",
       " ['their', 'theirs', 'theiring', 'theired'],\n",
       " ['feet', 'feets', 'feeting', 'feeted'],\n",
       " ['and', 'ands', 'anding', 'anded'],\n",
       " ['eat', 'eats', 'eating', 'ate', 'eaten'],\n",
       " ['best', 'bests', 'besting', 'bested'],\n",
       " ['fishes', 'fishing', 'fishesed'],\n",
       " ['andthe', 'andthes', 'andthing', 'andthed'],\n",
       " ['stripe', 'stripes', 'striping', 'striped'],\n",
       " ['bat', 'bats', 'batting', 'batted'],\n",
       " ['be',\n",
       "  'am',\n",
       "  'are',\n",
       "  'is',\n",
       "  'being',\n",
       "  'was',\n",
       "  'were',\n",
       "  'been',\n",
       "  'am not',\n",
       "  \"aren't\",\n",
       "  \"isn't\",\n",
       "  \"wasn't\",\n",
       "  \"weren't\"],\n",
       " ['hang', 'hangs', 'hanging', 'hung'],\n",
       " ['on', 'ons', 'oning', 'oned'],\n",
       " ['their', 'theirs', 'theiring', 'theired'],\n",
       " ['feet', 'feets', 'feeting', 'feeted'],\n",
       " ['and', 'ands', 'anding', 'anded'],\n",
       " ['eat', 'eats', 'eating', 'ate', 'eaten'],\n",
       " ['best', 'bests', 'besting', 'bested'],\n",
       " ['fishes', 'fishing', 'fishesed'],\n",
       " ['eatthe', 'eatthes', 'eatthing', 'eatthed'],\n",
       " ['stripe', 'stripes', 'striping', 'striped'],\n",
       " ['bat', 'bats', 'batting', 'batted'],\n",
       " ['be',\n",
       "  'am',\n",
       "  'are',\n",
       "  'is',\n",
       "  'being',\n",
       "  'was',\n",
       "  'were',\n",
       "  'been',\n",
       "  'am not',\n",
       "  \"aren't\",\n",
       "  \"isn't\",\n",
       "  \"wasn't\",\n",
       "  \"weren't\"],\n",
       " ['hang', 'hangs', 'hanging', 'hung'],\n",
       " ['on', 'ons', 'oning', 'oned'],\n",
       " ['their', 'theirs', 'theiring', 'theired'],\n",
       " ['feet', 'feets', 'feeting', 'feeted'],\n",
       " ['and', 'ands', 'anding', 'anded'],\n",
       " ['eat', 'eats', 'eating', 'ate', 'eaten'],\n",
       " ['best', 'bests', 'besting', 'bested'],\n",
       " ['fishes', 'fishing', 'fishesed'],\n",
       " ['bestthe', 'bestthes', 'bestthing', 'bestthed'],\n",
       " ['stripe', 'stripes', 'striping', 'striped'],\n",
       " ['bat', 'bats', 'batting', 'batted'],\n",
       " ['be',\n",
       "  'am',\n",
       "  'are',\n",
       "  'is',\n",
       "  'being',\n",
       "  'was',\n",
       "  'were',\n",
       "  'been',\n",
       "  'am not',\n",
       "  \"aren't\",\n",
       "  \"isn't\",\n",
       "  \"wasn't\",\n",
       "  \"weren't\"],\n",
       " ['hang', 'hangs', 'hanging', 'hung'],\n",
       " ['on', 'ons', 'oning', 'oned'],\n",
       " ['their', 'theirs', 'theiring', 'theired'],\n",
       " ['feet', 'feets', 'feeting', 'feeted'],\n",
       " ['and', 'ands', 'anding', 'anded'],\n",
       " ['eat', 'eats', 'eating', 'ate', 'eaten'],\n",
       " ['best', 'bests', 'besting', 'bested'],\n",
       " ['fishes', 'fishing', 'fishesed'],\n",
       " ['fishes', 'fishing', 'fishesed']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lexeme's for each word\n",
    "[lexeme(wd) for wd in sentence.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8DSbM6xp2yr"
   },
   "source": [
    "**Steps in NLP for preprocessing**\n",
    "\n",
    "1)Tokenization\n",
    "2)Stemming\n",
    "3)Lemmatization\n",
    "4)Part-of-speech (POS) tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9quNp27PzZeg",
    "outputId": "c44c1ea7-9bbb-472a-d3e9-7217fa818527"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all-nltk'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all-nltk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Creating token of words:\n",
      "['My', 'name', 'is', 'Adithya', 'Challa', 'I', 'wrote', 'this', 'shot', '!']\n",
      "\n",
      "\n",
      "Stemming:\n",
      "light\n",
      "light\n",
      "light\n",
      "\n",
      "\n",
      "Lemmatiztion:Converts allverb forms into root word:\n",
      "playing\n",
      "\n",
      "\n",
      "POS Tag:\n",
      "[('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Adithya', 'NNP'), ('Challa', 'NNP'), ('I', 'PRP'), ('wrote', 'VBD'), ('this', 'DT'), ('shot', 'NN'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "#Code to implement all the NLP preprocessing steps\n",
    "\n",
    "import nltk\n",
    "nltk.download('all-nltk')\n",
    "print(\"\\n\")\n",
    "\n",
    "# Creating token of words\n",
    "print(\"Creating token of words:\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "text=\"My name is Adithya Challa I wrote this shot!\"\n",
    "tokenize_word=word_tokenize(text)\n",
    "print(tokenize_word)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Stemming\n",
    "print(\"Stemming:\")\n",
    "from nltk.stem import PorterStemmer\n",
    "words=[\"light\",\"lighting\",\"lights\"]\n",
    "ps=PorterStemmer()\n",
    "for w in words:\n",
    "    rootword=ps.stem(w)\n",
    "    print(rootword)\n",
    "print(\"\\n\")\n",
    "\n",
    "#Lemmatiztion:Converts all verb forms into root word\n",
    "print(\"Lemmatiztion:Converts allverb forms into root word:\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lem=WordNetLemmatizer()\n",
    "print(lem.lemmatize(\"playing\"))\n",
    "print(\"\\n\")\n",
    "\n",
    "#POS Tag\n",
    "print(\"POS Tag:\")\n",
    "from nltk import word_tokenize,pos_tag\n",
    "text=\"My name is Adithya Challa I wrote this shot!\"\n",
    "print(pos_tag(word_tokenize(text)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7YkARONsNwM"
   },
   "source": [
    "***N-GRAMS***\n",
    "\n",
    "In the fields of Computational Linguistics an **n-gram** is a contiguous sequence of n items from a given sample of text or speech.\n",
    "\n",
    "The items can be letters, words or base pairs, phonemes or syllables   according to the application.\n",
    "\n",
    "Eg Consider a sentence:\n",
    "\n",
    "She was laughing at him.\n",
    "\n",
    "“she was” or “was laughing” etc are bigram(2-gram)\n",
    "“She was laughing” or “ laughing at him” etc are trigrams( 3-grams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yd0okG0fm3aT",
    "outputId": "c0876e36-1f43-4945-f0d5-41e4dba97dcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'reside')\n",
      "('reside', 'in')\n",
      "('in', 'Bengaluru.')\n"
     ]
    }
   ],
   "source": [
    "#Use of NLTK to print N-grams\n",
    "\n",
    "from nltk import ngrams\n",
    "sentence = 'I reside in Bengaluru.'\n",
    "n = 2\n",
    "unigrams = ngrams(sentence.split(), n)\n",
    "for grams in unigrams:\n",
    "  print(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emTab_kts-pp"
   },
   "source": [
    "**PORTER STEMMER**\n",
    "\n",
    "One of the most popular stemming algorithms is the Porter stemmer, which has been around since 1979."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z20N1xD8scFY",
    "outputId": "fa95aa28-12f5-49b5-f3bd-d1c86cc4fe8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program  :  program\n",
      "programs  :  program\n",
      "programmer  :  programm\n",
      "programming  :  program\n",
      "programmers  :  programm\n"
     ]
    }
   ],
   "source": [
    "# Stemming words\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# choose some words to be stemmed\n",
    "words = [\"program\", \"programs\", \"programmer\", \"programming\", \"programmers\"]\n",
    "\n",
    "for w in words:\n",
    "    print(w, \" : \", ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UqjWMASHtbyD",
    "outputId": "d955706e-6c00-49a7-b5d7-cd9093293f3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it\n",
      "is\n",
      "import\n",
      "to\n",
      "by\n",
      "veri\n",
      "pythonli\n",
      "while\n",
      "you\n",
      "are\n",
      "python\n",
      "with\n",
      "python\n",
      ".\n",
      "all\n",
      "python\n",
      "have\n",
      "python\n",
      "poorli\n",
      "at\n",
      "least\n",
      "onc\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Stemming a sentence\n",
    "\n",
    "new_text = \"It is important to by very pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.\"\n",
    "\n",
    "words = word_tokenize(new_text)\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_vj4uRatuZ5"
   },
   "source": [
    "**SNOWBALL STEMMER**\n",
    "\n",
    "It is a stemming algorithm which is also known as the Porter2 stemming algorithm as it is a better version of the Porter Stemmer since some issues of it were fixed in this stemmer.\n",
    "\n",
    "Difference Between Porter Stemmer and Snowball Stemmer:\n",
    "\n",
    "1)Snowball Stemmer is more aggressive than Porter Stemmer.\n",
    "\n",
    "2)Some issues in Porter Stemmer were fixed in Snowball Stemmer.\n",
    "\n",
    "3)There is only a little difference in the working of these two.\n",
    "\n",
    "4)Words like ‘fairly‘ and ‘sportingly‘ were stemmed to ‘fair’ and ‘sport’ in the snowball stemmer but when you use the porter stemmer they are stemmed to ‘fairli‘ and ‘sportingli‘.\n",
    "\n",
    "5)The difference between the two algorithms can be clearly seen in the way the word ‘Sportingly’ in stemmed by both. Clearly Snowball Stemmer stems it to a more accurate stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vS5BuvoptnTA",
    "outputId": "fed819dd-38d9-4e72-ab95-fe24b8b7c9b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cared ----> care\n",
      "university ----> univers\n",
      "fairly ----> fair\n",
      "easily ----> easili\n",
      "singing ----> sing\n",
      "sings ----> sing\n",
      "sung ----> sung\n",
      "singer ----> singer\n",
      "sportingly ----> sport\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#the stemmer requires a language parameter\n",
    "snow_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "#list of tokenized words\n",
    "words = ['cared','university','fairly','easily','singing',\n",
    "       'sings','sung','singer','sportingly']\n",
    "\n",
    "#stem's of each word\n",
    "stem_words = []\n",
    "for w in words:\n",
    "    x = snow_stemmer.stem(w)\n",
    "    stem_words.append(x)\n",
    "\n",
    "#print stemming results\n",
    "for e1,e2 in zip(words,stem_words):\n",
    "    print(e1+' ----> '+e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdLe2uJQyK2P"
   },
   "source": [
    "**Minimum Edit Distance Algorithm Code with example** (Dynamic programming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0yRTnMnGyRlf",
    "outputId": "d13bb742-84d5-4c9f-9ee5-0ddf49e47ff8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Operations required : 1\n",
      "No. of Operations required : 3\n"
     ]
    }
   ],
   "source": [
    "#Code to demonstrate the Minimum Edit Distance Algorithm\n",
    "def edit_distance(str1, str2, a, b):\n",
    "    string_matrix = [[0 for i in range(b+1)] for i in range(a+1)]\n",
    "\n",
    "    for i in range(a+1):\n",
    "        for j in range(b+1):\n",
    "\n",
    "            if i == 0:\n",
    "                string_matrix[i][j] = j   # If first string is empty, insert all characters of second string into first.\n",
    "\n",
    "            elif j == 0:\n",
    "                string_matrix[i][j] = i   # If second string is empty, remove all characters of first string.\n",
    "\n",
    "            elif str1[i-1] == str2[j-1]:\n",
    "                string_matrix[i][j] = string_matrix[i-1][j-1]  # If last characters of two strings are same, nothing much to do. Ignore the last two characters and get the count of remaining strings.\n",
    "\n",
    "            else:\n",
    "                string_matrix[i][j] = 1 + min(string_matrix[i][j-1],      # insert operation\n",
    "                                       string_matrix[i-1][j],      # remove operation\n",
    "                                       string_matrix[i-1][j-1])    # replace operation\n",
    "\n",
    "    return string_matrix[a][b]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    str1 = 'Cats'\n",
    "    str2 = 'Rats'\n",
    "\n",
    "    print('No. of Operations required :',edit_distance(str1, str2, len(str1), len(str2)))\n",
    "\n",
    "\n",
    "    str3 = 'Saturday'\n",
    "    str4 = 'Sunday'\n",
    "    print('No. of Operations required :',edit_distance(str3, str4, len(str3), len(str4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbM5OyIayall"
   },
   "source": [
    "**Explanation for the code above:**\n",
    "\n",
    "In Case-1, str1 =’Cats’ and str2 = ‘Rats’.  To change ‘Cats’ into ‘Rats’, only one update operation is required. That means letter ‘C’ is replaced by letter ‘R’.\n",
    "\n",
    "\n",
    "In Case-2 , str3 =’Saturday’ and str4=’Sunday’. To change ‘Saturday’ to ‘Sunday’, three operations are required. That means letters ‘a’ and ‘t’ are deleted and ‘n’ is inserted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDbklR0CvxXo"
   },
   "source": [
    "**Minimum Edit Distance to correct  misspelled words**\n",
    "\n",
    "Edit Distance measures dissimilarity between two strings by finding the minimum number of operations needed to transform one string into the other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q5yLp96vwIDN",
    "outputId": "69717da4-aff9-458b-f69d-717f5e04ceeb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy\n",
      "aiming\n",
      "intelligent\n"
     ]
    }
   ],
   "source": [
    "#Code to demonstrate Minimum Edit Distance to correct misspelled words\n",
    "from nltk.metrics.distance  import edit_distance\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "correct_words = words.words()\n",
    "\n",
    "incorrect_words=['happpy', 'azmaing', 'intelliengt']\n",
    "\n",
    "for word in incorrect_words:\n",
    "    temp = [(edit_distance(word, w),w) for w in correct_words if w[0]==word[0]]\n",
    "    print(sorted(temp, key = lambda val:val[0])[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oh6AZkL58E0g"
   },
   "source": [
    "**N GRAM LANGUAGE MODEL**\n",
    "\n",
    "An N-gram language model predicts the probability of a given N-gram within any sequence of words in the language.\n",
    "\n",
    "If we have a good N-gram model, we can predict p(w | h) – what is the probability of seeing the word w given a history of previous words h – where the history contains n-1 words.\n",
    "\n",
    "\n",
    "*We compute this probability in two steps:*\n",
    "\n",
    "1)Apply the chain rule of probability.\n",
    "\n",
    "2)We then apply a very strong simplification assumption to allow us to compute p(w1…ws) in an easy manner\n",
    "\n",
    "\n",
    "Now that we understand what an N-gram is, let’s build a basic language model using trigrams of the Reuters corpus. Reuters corpus is a collection of 10,788 news documents totaling 1.3 million words.\n",
    "\n",
    "\n",
    "We first split our text into trigrams with the help of NLTK and then calculate the frequency in which each combination of the trigrams occurs in the dataset.\n",
    "\n",
    "We then use it to calculate probabilities of a word, given the previous two words. That’s essentially what gives us our Language Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l--X7SFz-IC5",
    "outputId": "8cef6cad-6bd6-4c36-e84c-2ec0530530eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'public': 0.05555555555555555, 'European': 0.05555555555555555, 'Bank': 0.05555555555555555, 'price': 0.1111111111111111, 'emirate': 0.05555555555555555, 'overseas': 0.05555555555555555, 'newspaper': 0.05555555555555555, 'company': 0.16666666666666666, 'Turkish': 0.05555555555555555, 'increase': 0.05555555555555555, 'options': 0.05555555555555555, 'Higher': 0.05555555555555555, 'pound': 0.05555555555555555, 'Italian': 0.05555555555555555, 'time': 0.05555555555555555}\n"
     ]
    }
   ],
   "source": [
    "# code courtesy of https://nlpforhackers.io/language-models/\n",
    "\n",
    "from nltk.corpus import reuters\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter, defaultdict\n",
    "import nltk\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Create a placeholder for model\n",
    "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "# Count frequency of co-occurance\n",
    "for sentence in reuters.sents():\n",
    "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "        model[(w1, w2)][w3] += 1\n",
    "\n",
    "# Let's transform the counts to probabilities\n",
    "for w1_w2 in model:\n",
    "    total_count = float(sum(model[w1_w2].values()))\n",
    "    for w3 in model[w1_w2]:\n",
    "        model[w1_w2][w3] /= total_count\n",
    "\n",
    "print(dict(model['today', 'the']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dWDm2avJe9Y"
   },
   "source": [
    "If we keep following this process iteratively, we will soon have a coherent sentence! Here is a script to play around with generating a random piece of text using our n-gram model:\n",
    "\n",
    "This is the same underlying principle which the likes of Google, Alexa, and Apple use for language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0JK0DLWk--TJ",
    "outputId": "78dafb94-1345-41d0-f9e4-81598c2bdfeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "today the newspaper article .\n"
     ]
    }
   ],
   "source": [
    "#generating a random piece of text using our n-gram model\n",
    "\n",
    "import random\n",
    "\n",
    "# starting words\n",
    "text = [\"today\", \"the\"]\n",
    "sentence_finished = False\n",
    "\n",
    "while not sentence_finished:\n",
    "  # select a random probability threshold\n",
    "  r = random.random()\n",
    "  accumulator = .0\n",
    "\n",
    "  for word in model[tuple(text[-2:])].keys():\n",
    "      accumulator += model[tuple(text[-2:])][word]\n",
    "      # select words that are above the probability threshold\n",
    "      if accumulator >= r:\n",
    "          text.append(word)\n",
    "          break\n",
    "\n",
    "  if text[-2:] == [None, None]:\n",
    "    sentence_finished = True\n",
    "\n",
    "print (' '.join([t for t in text if t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B008VEjQJRtK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
