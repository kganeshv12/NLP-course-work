{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MASTER\n",
    "# EXPLORATION OF NLTK TOOLKIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPLORATION OF NLTK TOOLKIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-Levenshtein in c:\\users\\bhavani k\\appdata\\roaming\\python\\python310\\site-packages (0.23.0)\n",
      "Requirement already satisfied: Levenshtein==0.23.0 in c:\\users\\bhavani k\\appdata\\roaming\\python\\python310\\site-packages (from python-Levenshtein) (0.23.0)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.1.0 in c:\\users\\bhavani k\\appdata\\roaming\\python\\python310\\site-packages (from Levenshtein==0.23.0->python-Levenshtein) (3.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-Levenshtein\n",
    "!pip install -U spacy\n",
    "!pip install scipy\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Bhavani\n",
      "[nltk_data]     K\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Bhavani\n",
      "[nltk_data]     K\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Bhavani K\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Bhavani\n",
      "[nltk_data]     K\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30+20+10   title and IEEE -> 10 marks\n",
    "# 2.5 - tutorial 1\n",
    "# 2.5 - tutorial 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jai Shree Ram!', 'Jai .', 'shree .', 'Ram .']\n"
     ]
    }
   ],
   "source": [
    "text  = \"Jai Shree Ram! Jai . shree . Ram .\"\n",
    "list1 = sent_tokenize(text)\n",
    "print(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jai', 'Shree', 'Ram', '!', 'Jai', '.', 'shree', '.', 'Ram', '.']\n"
     ]
    }
   ],
   "source": [
    "text  = \"Jai Shree Ram! Jai . shree . Ram .\"\n",
    "list2 = word_tokenize(text)\n",
    "print(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "text  = \"Ram's back to Ayodhya!\"\n",
    "list2 = word_tokenize(text)\n",
    "print(list2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINDING THE CONTENT WORDS AND THE FUNCTION WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jai', 'Shree', 'Ram', '!', 'Jai', '.', 'shree', '.', 'Ram', '.']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words())\n",
    "content_words = [words for words in list2 if words.lower() not in stop_words]\n",
    "# vocabulary is present in english LOWERCASE\n",
    "function_words = [words for words in list2 if words.lower() in stop_words]\n",
    "print(content_words)\n",
    "print(function_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the text like \n",
    "# take 2 lines consider the stp words like  one '.' or and, in '...' to understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content words: ['Reasoning', 'reliability', 'AI', 'Leveraging', 'language', 'machine']\n"
     ]
    }
   ],
   "source": [
    "stop_temp = ['AND', 'TO', '...', '.', 'ONE', 'OR', 'IN', 'UNDERSTAND']\n",
    "\n",
    "text = \"Reasoning and reliability in AI. ... Leveraging language to understand machine\"\n",
    "\n",
    "list3 = word_tokenize(text)\n",
    "content_words_next = [words for words in list3 if words.upper() not in stop_temp]\n",
    "print(\"content words:\", content_words_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEMMING IN NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEMMER SUPPORTED BY NLTK - RegexpStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning stem: reason\n",
      "reliability stem: reliabl\n",
      "AI stem: ai\n",
      "Leveraging stem: leverag\n",
      "language stem: languag\n",
      "machine stem: machin\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "PS = PorterStemmer()\n",
    "\n",
    "for w in content_words_next:\n",
    "    print(w, \"stem:\", PS.stem(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning stem: reason\n",
      "reliability stem: reliabl\n",
      "AI stem: ai\n",
      "Leveraging stem: leverag\n",
      "language stem: languag\n",
      "machine stem: machin\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "SS = SnowballStemmer(language=\"english\")\n",
    "for w in content_words_next:\n",
    "    print(w, \"stem:\", SS.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning stem: reason\n",
      "reliability stem: rely\n",
      "AI stem: ai\n",
      "Leveraging stem: lev\n",
      "language stem: langu\n",
      "machine stem: machin\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "LS = LancasterStemmer()\n",
    "for w in content_words_next:\n",
    "    print(w, \"stem:\", LS.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'عمل'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ARABIC\n",
    "from nltk.stem.arlstem import ARLSTem\n",
    "stemmer = ARLSTem()\n",
    "stemmer.stem('يعمل')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Bhavani\n",
      "[nltk_data]     K\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning stem: Reasoning\n",
      "reliability stem: reliability\n",
      "AI stem: AI\n",
      "Leveraging stem: Leveraging\n",
      "language stem: language\n",
      "machine stem: machine\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "for w in content_words_next:\n",
    "    print(w, \"stem:\", lem.lemmatize(w))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-GRAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi [('h',), ('i',)] [('h', 'i')] []\n",
      "hello [('h',), ('e',), ('l',), ('l',), ('o',)] [('h', 'e'), ('e', 'l'), ('l', 'l'), ('l', 'o')] [('h', 'e', 'l'), ('e', 'l', 'l'), ('l', 'l', 'o')]\n",
      "! [('!',)] [] []\n",
      "NLP [('N',), ('L',), ('P',)] [('N', 'L'), ('L', 'P')] [('N', 'L', 'P')]\n",
      "class [('c',), ('l',), ('a',), ('s',), ('s',)] [('c', 'l'), ('l', 'a'), ('a', 's'), ('s', 's')] [('c', 'l', 'a'), ('l', 'a', 's'), ('a', 's', 's')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "sen = \" hi hello! NLP class\"\n",
    "words_grams = word_tokenize(sen)\n",
    "for x in words_grams:\n",
    "    print(x, list(ngrams(x,1)), list(ngrams(x,2)), list(ngrams(x,3)))\n",
    "\n",
    "    # for unigram bi gram and tri gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MINIMUM EDIT DISTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  HELPS CHANGE THE WORD FORM ONE WORD TO ANOTHER BY USING THIS DISTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import edit_distance\n",
    "str1 = \"SHAURYADEEP\"\n",
    "str2 = \"GANESH\"\n",
    "# dist = edit_distance(str1, str2, substitution_cost=1)\n",
    "dist = edit_distance(str1, str2, substitution_cost=2)\n",
    "print(dist)\n",
    "\n",
    "# here we get 2 becos substitution cost is 1 not 2 like we took in class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 1, 0], [1, 2, 2, 2, 1], [2, 1, 3, 3, 2], [2, 2, 3, 3, 3], [2, 2, 3, 3, 3], [2, 2, 3, 3, 3]]\n",
      "Minimum edit distance -  3\n"
     ]
    }
   ],
   "source": [
    "# define your own function of edit distance using dynamic programming in the class\n",
    "n = 5\n",
    "m = 6\n",
    "\n",
    "\n",
    "arr=[]\n",
    "\n",
    "for i in range(m):\n",
    "    col = []\n",
    "    for j in range(n):\n",
    "        col.append(0)\n",
    "    arr.append(col)\n",
    "# print(arr)\n",
    "\n",
    "# print(m1)\n",
    "i = 0\n",
    "\n",
    "for j in range(1,n):\n",
    "    arr[0][j]  = j\n",
    "    \n",
    "for j in range(1,m):\n",
    "    arr[j][0] = j\n",
    "\n",
    "\n",
    "for i in range(m):\n",
    "    for j in range(n):\n",
    "        if str1[i] == str2[j] :\n",
    "            cost = 0 \n",
    "        else:\n",
    "            cost = 2\n",
    "        arr[i][j] = min(arr[1][j-1]+1,arr[i-1][j]+1,arr[i-1][j-1]+cost)\n",
    "\n",
    "\n",
    "print(arr)\n",
    "print(\"Minimum edit distance - \",arr[m-1][n-1])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEMMING AND LEMMATIZATION IN SPACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPACY DOES NOT PROVIDE AN INBUILT METHOD TO PERFORM STEMMING OF TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization lemmatization\n",
      "using use\n",
      "spacy spacy\n",
      "! !\n"
     ]
    }
   ],
   "source": [
    "# LEMMATIZATION\n",
    "import spacy \n",
    "# !python -m spacy download en_core_web_sm\n",
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "sentence1 = sp(u'Lemmatization using spacy!')\n",
    "for word in sentence1:\n",
    "    print(word.text,  word.lemma_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOKENIZATION IN GENSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: This is a sample text for tokenization using Gensim! It's cool?\n",
      "Tokenized text: ['this', 'is', 'sample', 'text', 'for', 'tokenization', 'using', 'gensim', 'it', 'cool']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "text = \"This is a sample text for tokenization using Gensim! It's cool?\"\n",
    "tokenized = []\n",
    "for word in text.split():\n",
    "    tokenized.extend(simple_preprocess(word))\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"Tokenized text:\", tokenized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference between the tokenizer of NLTK and Gensim is that NLTK uses a regular expression to split the text into tokens, while Gensim uses a statistical model.\n",
    "<br>\n",
    "Gensim's tokenizer is a statistical model that uses a language model to predict the next word in a sequence. It is a good choice for tasks such as topic modeling and document similarity.\n",
    "<br>\n",
    "NLTK's tokenizer is a simple rule-based tokenizer that splits text into tokens based on whitespace, punctuation, and other predefined rules. It is a good choice for general-purpose tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEXTBLOB - NLP LIBRARY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out how good the TextBlob is in finding the edit distance and finding the ngrams with respect to NLTK.<br>\n",
    "We have done edit diatnace and ngrams in NLTK above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min Edit Distance <br>\n",
    "TEXTBLOB does not have a builtin function for calculating minimum edit distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextBlob 1-grams: [WordList(['This']), WordList(['is']), WordList(['an']), WordList(['example']), WordList(['sentence'])]\n",
      "TextBlob 2-grams: [WordList(['This', 'is']), WordList(['is', 'an']), WordList(['an', 'example']), WordList(['example', 'sentence'])]\n",
      "TextBlob 3-grams: [WordList(['This', 'is', 'an']), WordList(['is', 'an', 'example']), WordList(['an', 'example', 'sentence'])]\n"
     ]
    }
   ],
   "source": [
    "# NGRAMS IN TEXTBLOB\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "text = \"This is an example sentence.\"\n",
    "\n",
    "blob = TextBlob(text)\n",
    "ngrams = blob.ngrams(n=1)\n",
    "print(\"TextBlob 1-grams:\", ngrams)\n",
    "ngrams = blob.ngrams(n=2)\n",
    "print(\"TextBlob 2-grams:\", ngrams)\n",
    "ngrams = blob.ngrams(n=3)\n",
    "print(\"TextBlob 3-grams:\", ngrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob is built on top of NLTK and provides a simpler interface for common NLP tasks, including sentiment analysis, part-of-speech tagging, and more. However, when it comes to specific tasks like finding edit distance and generating n-grams, NLTK provides more fine-grained control and additional functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, TextBlob tokenizes the input text into words and then generates n-grams by iterating through the tokenized words. It uses Python's built-in zip function to create tuples of words at different positions in the tokenized sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PATTERN - NLP LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This   sample sentence with stopwords.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_stopwords(text):\n",
    "  stopwords = set([\"the\", \"is\", \"are\", \"of\", \"and\", \"a\", \"in\", \"to\"])\n",
    "  pattern = re.compile(r\"\\b(?:{})\\b\".format(\"|\".join(stopwords)))\n",
    "  return pattern.sub(\"\", text)\n",
    "\n",
    "text = \"This is a sample sentence with stopwords.\"\n",
    "print(remove_stopwords(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
